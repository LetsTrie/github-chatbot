# start_time = time.time()

# end_time = time.time()
# elapsed_time = end_time - start_time
# print(f"Time taken to fetch initial report data: {elapsed_time:.4f} seconds")

# print(json.dumps(report_data, indent=4))

# github_url = "https://github.com/freeCodeCamp/freeCodeCamp"
# github_url = "https://github.com/midday-ai/midday"
# github_url = "https://github.com/jonogon/jonogon-mono"

# owner, repo = get_repo_from_url(github_url)

# metadata = { "owner": owner, "repo": repo }
# print(json.dumps(metadata))

# from langchain.globals import set_debug
# set_debug(True)

# github_url = "https://github.com/apache/kafka"
# github_url = "https://github.com/midday-ai/v1"
# github_url = "https://github.com/midday-ai/midday"
# github_url = "https://github.com/LetsTrie/Grind-75"
# github_url = "https://github.com/jonogon/jonogon-mono"
# github_url = "https://github.com/OptimalBits/bull"
# github_url = "https://github.com/freeCodeCamp/freeCodeCamp"

# !pyenv install 3.12.4
# !pyenv local 3.12.4
# !python -m venv .venv
# !source .venv/bin/activate

# DATA_DIRECTORY = "data"
# os.makedirs(DATA_DIRECTORY, exist_ok=True)

# def create_csv_file(data_list, file_name):
#     file_path = os.path.join(DATA_DIRECTORY, file_name)
#     with open(file_path, mode='w', newline='') as csv_file:
#         csv_writer = csv.DictWriter(csv_file, fieldnames=data_list[0].keys())
#         csv_writer.writeheader()
#         csv_writer.writerows(data_list)
#     logging.info(f"CSV file '{file_path}' created successfully.")

# def read_csv_to_array(file_name):
#     data_list = []
#     file_path = os.path.join(DATA_DIRECTORY, file_name)
    
#     try:
#         with open(file_path, mode='r') as csv_file:
#             csv_reader = csv.DictReader(csv_file)
#             for row in csv_reader:
#                 data_list.append(row)
#         logging.info(f"Data successfully read from '{file_path}' into array.")
#     except FileNotFoundError:
#         logging.error(f"File '{file_path}' not found.")
#     except Exception as error:
#         logging.error(f"An error occurred while reading '{file_path}': {error}")
    
#     return data_list

# def get_data_with_backup(filename, cb):
#     csv_path = os.path.join(DATA_DIRECTORY, filename)
    
#     if os.path.exists(csv_path) and is_backup_required:
#         logging.info(f"File '{csv_path}' exists. Reading data from the file.")
#         return read_csv_to_array(filename)
#     else:
#         logging.info(f"File '{csv_path}' does not exist. Executing GitHub API.")
#         data = cb()
#         if data:
#             create_csv_file(data, filename)
#         return data

# !pip install openai

# import os
# from openai import OpenAI

# client = OpenAI(
#     # This is the default and can be omitted
#     api_key="",
# )

# chat_completion = client.chat.completions.create(
#     messages=[
#         {
#             "role": "user",
#             "content": "Say this is a test",
#         }
#     ],
#     model="gpt-3.5-turbo",
# )

# print(chat_completion)

# import requests
# import json

# GITHUB_TOKEN = GITHUB_ACCESS_TOKEN
# GITHUB_API_URL = "https://api.github.com/graphql"

# def get_issue_timeline(owner, repo, issue_number):
#     timeline_items = []
#     has_next_page = True
#     cursor = None
    
#     while has_next_page:
#         query = f"""
#         {{
#           repository(owner: "{owner}", name: "{repo}") {{
#             issue(number: {issue_number}) {{
#               title
#               body
#               comments(first: 100) {{
#                 nodes {{
#                   bodyText
#                   author {{
#                     login
#                   }}
#                 }}
#               }}
#               timelineItems(first: 100{', after: "' + cursor + '"' if cursor else ''}) {{
#                 pageInfo {{
#                   hasNextPage
#                   endCursor
#                 }}
#                 nodes {{
#                   __typename
#                   ... on ClosedEvent {{
#                     id
#                     createdAt
#                     url
#                     closer {{
#                       __typename
#                       ... on Commit {{
#                         message
#                         url
#                       }}
#                       ... on PullRequest {{
#                         number
#                         title
#                         url
#                       }}
#                     }}
#                   }}
#                   ... on CrossReferencedEvent {{
#                     actor {{
#                       login
#                     }}
#                     createdAt
#                     id
#                     isCrossRepository
#                     referencedAt
#                     resourcePath
#                     url
#                     willCloseTarget
#                     source {{
#                       __typename
#                       ... on PullRequest {{
#                         number
#                         title
#                         url
#                       }}
#                       ... on Issue {{
#                         number
#                         title
#                         url
#                       }}
#                     }}
#                     target {{
#                       __typename
#                       ... on PullRequest {{
#                         number
#                         title
#                         url
#                       }}
#                       ... on Issue {{
#                         number
#                         title
#                         url
#                       }}
#                     }}
#                   }}
#                 }}
#               }}
#             }}
#           }}
#         }}
#         """

#         headers = {
#             "Authorization": f"Bearer {GITHUB_TOKEN}"
#         }

#         response = requests.post(GITHUB_API_URL, json={"query": query}, headers=headers)

#         if response.status_code == 200:
#             data = response.json()
#             issue_data = data["data"]["repository"]["issue"]
#             timeline_data = issue_data["timelineItems"]
#             timeline_items.extend(timeline_data["nodes"])  # Collect the nodes
            
#             page_info = timeline_data["pageInfo"]
#             has_next_page = page_info["hasNextPage"]
#             cursor = page_info["endCursor"]
            
#         else:
#             print(f"Query failed with status code {response.status_code}: {response.text}")
#             break

#     return timeline_items

# owner = "freeCodeCamp"
# repo = "freeCodeCamp"
# issue_number = 42256

# timeline_items = get_issue_timeline(owner, repo, issue_number)
# print(json.dumps(timeline_items, indent=4))

from datetime import datetime

def process_timeline_with_metadata(timeline):
    processed_events = []

    for event in timeline:
        processed_event = {
            "event_type": event.get("event"),
            "actor_login": event["actor"]["login"] if event.get("actor") else None,
            "actor_id": event["actor"]["id"] if event.get("actor") else None,
            "actor_avatar_url": event["actor"]["avatar_url"] if event.get("actor") else None,
            "timestamp": datetime.strptime(event["created_at"], "%Y-%m-%dT%H:%M:%SZ"),
            "event_url": event.get("url"),
            "performed_via_github_app": event.get("performed_via_github_app", False)
        }

        if event.get("event") == "commented":
            processed_event['metadata'] = {
                "comment_body": event.get("body"),
                "comment_id": event.get("id")
            }
        
        elif event.get("event") == "labeled":
            processed_event['metadata'] = {
                "label_name": event.get("label", {}).get("name"),
                "label_color": event.get("label", {}).get("color")
            }
        
        elif event.get("event") == "cross-referenced":
            source_issue = event.get("source", {}).get("issue", {})
            pull_request = source_issue.get("pull_request")
            
            if pull_request:
                processed_event['metadata'] = {
                    "source_issue_url": source_issue.get("html_url"),
                    "pull_request_url": pull_request.get("html_url"),
                    "pull_request_number": source_issue.get("number"),
                    "repository": source_issue.get("body", ""),
                    "source_issue": source_issue
                }
        
        elif event.get("event") == "assigned":
            processed_event['metadata'] = {
                "assignee": event.get("assignee", {}).get("login"),
                "assigner": event.get("assigner", {}).get("login")
            }
        
        elif event.get("event") == "unassigned":
            processed_event['metadata'] = {
                "unassignee": event.get("assignee", {}).get("login"),
                "unassigner": event.get("assigner", {}).get("login")
            }
        
        elif event.get("event") == "closed":
            processed_event['metadata'] = {
                "closed_by": event.get("actor", {}).get("login"),
                "commit_id": event.get("commit_id"),
                "commit_url": event.get("commit_url")
            }
        
        elif event.get("event") == "reopened":
            processed_event['metadata'] = {
                "reopened_by": event.get("actor", {}).get("login"),
                "created_at": event.get("created_at")
            }
        
        elif event.get("event") == "merged":
            processed_event['metadata'] = {
                "merged_by": event.get("actor", {}).get("login"),
                "commit_id": event.get("commit_id"),
                "commit_url": event.get("commit_url")
            }
        
        elif event.get("event") == "milestoned":
            processed_event['metadata'] = {
                "milestone_title": event.get("milestone", {}).get("title"),
                "actor": event.get("actor", {}).get("login")
            }
        
        elif event.get("event") == "review_requested":
            processed_event['metadata'] = {
                "reviewer": event.get("requested_reviewer", {}).get("login"),
                "requested_by": event.get("review_requester", {}).get("login")
            }

        elif event.get("event") == "connected":
            processed_event['metadata'] = {
                "issue_url": event.get("source", {}).get("issue", {}).get("html_url"),
                "pull_request_url": event.get("source", {}).get("pull_request", {}).get("html_url")
            }

        processed_events.append(processed_event)

    processed_events.sort(key=lambda x: x["timestamp"])
    return processed_events


# timeline = process_timeline_with_metadata(fetch_github_data(endpoint="issues/42256/timeline", repo=f"{owner}/{repo}"))
# print(json.dumps(timeline, indent=4, sort_keys=True, default=str))

Every pull request is an issue, but not every issue is a pull request. For this reason, "shared" actions for both features, like managing assignees, labels, and milestones, are provided within the Issues endpoints.

1. The node ID typically starts with PR*, signaling it's a pull request. The node ID typically starts with I*, signaling it's an issue.
2. Both pull requests and issues have a timeline_url to track the events associated with them (comments, status changes, etc.).
3. The html_url contains /pull/ in the path, indicating it is a pull request. The html_url contains /issues/, indicating it is an issue.
4. Presence of the `pull_request` key. `pull_request` should be `None` for issues and should exist for pull requests

# pr_review_comments, pr_conversations = get_pr_messages(owner,repo)
# print(json.dumps(pr_review_comments, indent = 4))
# print(json.dumps(pr_conversations, indent = 4))


# def get_all_pr_data(owner, repo, pr_numbers):
#     pr_review_comments = []
#     pr_conversations = []

#     for pr_number in pr_numbers:
#         pr_review_comments.extend(get_pr_review_comments(owner, repo, pr_number))
#         pr_conversations.extend(get_pr_conversations(owner, repo, pr_number))

#     return pr_review_comments, pr_conversations

# def get_pr_messages(owner, repo):
#     pr_review_comments = []
#     pr_conversations = []
    
#     review_comments_file = f"{owner}_{repo}_pr_review_comments.csv"
#     pr_conversations_file = f"{owner}_{repo}_pr_conversations.csv"

#     is_review_comments_file_exists = os.path.exists(os.path.join(DATA_DIRECTORY, review_comments_file))
#     is_pr_conversations_file_exists = os.path.exists(os.path.join(DATA_DIRECTORY, pr_conversations_file))

#     if is_review_comments_file_exists and is_pr_conversations_file_exists and is_backup_required:
#         logging.info(f"File '{review_comments_file}' exists. Reading data from the file.")
#         pr_review_comments = read_csv_to_array(review_comments_file)
#         pr_conversations = read_csv_to_array(pr_conversations_file)
#     else:
#         logging.info(f"File '{review_comments_file}' does not exist. Fetching data from the GitHub API.")
#         pr_review_comments, pr_conversations = get_all_pr_data(owner, repo, pr_numbers)
#         create_csv_file(pr_review_comments, review_comments_file)
#         create_csv_file(pr_conversations, pr_conversations_file)
#         logging.info(f"{review_comments_file} and {pr_conversations_file} created with backup data.")
    
#     return pr_review_comments, pr_conversations
    
# def extract_review_comment_data(review_comment):
#     return {
#         "comment_url": review_comment["url"],
#         "review_id": review_comment["pull_request_review_id"],
#         "comment_id": review_comment["id"],
#         "file_path": review_comment["path"],
#         "comment_body": review_comment["body"],
#         "author_login": review_comment["user"]["login"],
#         "author_url": review_comment["user"]["html_url"],
#         "created_at": review_comment["created_at"],
#         "updated_at": review_comment["updated_at"],
#         "pr_url": review_comment["pull_request_url"]
#     }

# def extract_conversation_data(conversation):
#     return {
#         "comment_url": conversation["url"],
#         "html_url": conversation["html_url"],
#         "author_login": conversation["user"]["login"],
#         "author_url": conversation["user"]["html_url"],
#         "created_at": conversation["created_at"],
#         "updated_at": conversation["updated_at"],
#         "comment_body": conversation["body"]
#     }

# def get_pr_review_comments(owner, repo, pull_number):
#     review_comments = fetch_github_data(endpoint=f"pulls/{pull_number}/comments", repo=f"{owner}/{repo}")
#     if review_comments is not None:
#         return [extract_review_comment_data(comment) for comment in review_comments]
#     else:
#         return []

# def get_pr_conversations(owner, repo, pull_number):
#     conversations = fetch_github_data(endpoint=f"issues/{pull_number}/comments", repo=f"{owner}/{repo}")
#     if conversations is not None:
#         return [extract_conversation_data(conversation) for conversation in conversations]
#     else:
#         return []

# pr_numbers = []

# def fetch_and_process_pull_requests(owner, repo):
#     pull_requests = fetch_github_data(endpoint="pulls", repo=f"{owner}/{repo}")

#     processed_prs = []
#     for pr in pull_requests: 
#         processed_prs.append({
#             "pr_title": pr["title"],
#             "pr_url": pr["html_url"],
#             "pr_number": pr["number"],
#             "state": pr["state"],
#             "author_name": pr["user"]["login"],
#             "author_url": pr["user"]["html_url"],
#             "created_at": pr["created_at"],
#             "updated_at": pr["updated_at"],
#             "closed_at": pr["closed_at"],
#             "merged_at": pr["merged_at"],
#             "assignee_name": pr["assignee"]["login"] if pr["assignee"] else None,
#             "assignee_url": pr["assignee"]["html_url"] if pr["assignee"] else None,
#             "requested_reviewers": [
#                 {"reviewer_name": reviewer["login"], "reviewer_url": reviewer["html_url"]}
#                 for reviewer in pr["requested_reviewers"]
#             ],
#             "labels": [label["name"] for label in pr["labels"]],
#             "milestone_title": pr["milestone"]["title"] if pr["milestone"] else None,
#             "commit_sha": pr["head"]["sha"],
#             "commit_ref": pr["head"]["ref"],
#             "base_branch": pr["base"]["ref"],
#             "description": pr["body"],
#             "review_comments_url": pr["review_comments_url"],
#             "commits_url": pr["commits_url"],
#             "statuses_url": pr["statuses_url"],
#             "pr_author_association": pr["author_association"],
#             "__main_from_pr_list": pr
#         })
#         pr_numbers.append(pr["number"])
    
#     return processed_prs

# def get_pull_requests(owner, repo):
#     return get_data_with_backup(f"{owner}_{repo}_pull_requests.csv", lambda: fetch_and_process_pull_requests(owner, repo))

# # pull_requests = get_pull_requests(owner, repo)

# last_30_days = (datetime.now() - timedelta(days=30)).isoformat() + "Z"
# last_7_days = (datetime.now() - timedelta(days=7)).isoformat() + "Z"

# Plan

- Get repository basic informations.
  [Challenges: Get count of (open/close) (commits/pull requests/issues) without fetching them all]
  [TODO: work on current version]
- Get last 30 days commits/pull requests/issues data to answer on latest data.
- Fine tune model

### TODO

1. Work on getting the current version.
2. Chat context understanding...
3. Find how many files are updated in a commit...
4. Issue and PR details api call "on_demand"...
5. Issue and PR timeline api call "on_demand"...
6. Branch details on an commit/pr...
7. Challenges: get count of commits/prs/issues without fetching them all...
8. get branch details and update...


# start_time = time.time()

# params = {"state": "all", "since": last_30_days} 
# issues_and_prs = get_issues_and_pull_requests(params=params)

# print(json.dumps(issues_and_prs, indent=4))

# end_time = time.time()
# elapsed_time = end_time - start_time
# print(f"Time taken to fetch issues and pull requests: {elapsed_time:.4f} seconds")


# # Every pull request is an issue, but not every issue is a pull request. For this reason, "shared" actions for both features.

# # 1. The node ID typically starts with PR_, signaling it's a pull request. The node ID typically starts with I_, signaling it's an issue.
# # 2. Both pull requests and issues have a timeline_url to track the events associated with them (comments, status changes, etc.).
# # 3. The html_url contains /pull/ in the path, indicating it is a pull request. The html_url contains /issues/, indicating it is an issue.
# # 4. Presence of the pull_request key. pull_request should be None for issues and should exist for pull requests

# def get_issues_and_pull_requests(params=None):
#     issues_data = fetch_github_data(endpoint="issues", params=params)

#     if not issues_data:
#         logging.error("Failed to fetch issues from the GitHub API.")
#         return []

#     processed_issues = []
#     for issue in issues_data:
#         processed_issues.append({
#             "number": issue["number"],
#             "title": issue["title"],
#             "html_url": issue["html_url"],
#             "state": issue["state"],
#             "state_reason": issue.get("state_reason"),
#             "created_at": issue["created_at"],
#             "updated_at": issue["updated_at"],
#             "closed_at": issue.get("closed_at"),
#             "author_name": issue["user"]["login"],
#             "closed_by": issue.get("closed_by", {}).get("login"),
#             "labels": [label["name"] for label in issue["labels"]],
#             "assignee_name": issue["assignee"]["login"] if issue.get("assignee") else None,
#             "description": issue.get("body"),
#             "reactions": issue.get("reactions"),
#             "comments": issue["comments"],
#             "locked": issue["locked"]
#         })

#     return processed_issues

# # html_url = https://github.com/<owner>/<repo>/<issues or pull>/<number>
# # author_url = https://github.com/<author_name>
# # pull_request = https://github.com/<owner>/<repo>/pull/<number>

# async def get_initial_report_data():
#     repository, commit_count, commit_count_in_past_week, issue_count, pull_request_count, contributors = await asyncio.gather(
#         get_repository(),
#         fetch_github_count("commits"),
#         fetch_github_count("commits"),
#         fetch_github_count("issues", params={"state": "open"}),
#         fetch_github_count("pulls", params={"state": "open"}),
#         fetch_github_count("contributors")
#     )
    
#     return {
#         "repository": repository,
#         "metrics": {
#             "contributors": contributors,
#             "commit_count": commit_count,
#             "recent_commits_in_week": commit_count_in_past_week,
#             "open_issues": issue_count,
#             "open_pull_requests": pull_request_count
#         },
#     }

# template = """
# You are a GitHub repository code reviewer chatbot. 
# Using the metadata below, create a concise report with key metrics. Add appropriate emojis. End with a 100-word summary.
# Start the report with: "Let‚Äôs dive into the key details of the repository "{repository[name]}"." 

# Repository Overview:
# - Name: {repository[name]}
# - Description: {repository[description]}
# - Owner: {repository[owner]}
# - Visibility: {repository[visibility]}
# - Homepage: {repository[homepage]}
# - Created: {repository[created_at]}
# - Last Pushed: {repository[last_pushed]}
# - Default Branch: {repository[default_branch]}
# - Primary Language: {repository[language]}
# - License: {repository[license]}

# Metrics:
# - Stars: {repository[stars]}
# - Watchers: {repository[watchers]}
# - Forks: {repository[forks]}
# - Subscribers: {repository[subscribers]}
# - Open Issues: {repository[open_issues]}, Open PRs: {metrics[open_pull_requests]}
# - Total Contributors: {metrics[contributors]}, Commits: {metrics[commit_count]} (Last 7 days: {metrics[recent_commits_in_week]})
# """

# prompt = PromptTemplate(
#     input_variables=["repository", "metrics"], template=template
# )

# class OutputBufferManager:
#     def __init__(self):
#         self.output_buffer = []

#     def clear_output(self): 
#         clear_output(wait=False)

#     def display_from_buffer(self):
#         clear_output(wait=False)
        
#         for output in self.output_buffer:
#             display(output)

#     def display_content(self, content):
#         markdown_content = Markdown(content)
#         self.output_buffer.append(markdown_content)
#         display(markdown_content)

#     def show_loader(self, content="Generating report..."):
#         self.display_content(f"ü§ñ **Git-Bot:** ‚è≥ {content}")

#     def hide_loader(self):
#         if self.output_buffer:
#             self.output_buffer.pop()
            
#         self.display_from_buffer()

#     def display_gitbot_response(self, content):
#         self.display_content(f"ü§ñ **Git-Bot:** {content}")

#     def display_gitbot_logs(self, content):
#         self.display_content(f"ü™µ **Git-Bot Logs:** {content}")
# buffer_manager = OutputBufferManager()

# model = ChatOpenAI(model="gpt-4o")

# get github url
# github_url = input("User: Please enter the GitHub Repository URL: ")
# github_url = "https://github.com/freeCodeCamp/freeCodeCamp"

# buffer_manager.clear_output()
# buffer_manager.display_content(f"üìÇ **Repository:** {github_url}")

# update metadata
# owner, repo = get_repo_from_url(github_url)
# metadata = { "owner": owner, "repo": repo }

# # get data from github API
# report_data = await get_initial_report_data()

# # initial report prompt template
# initial_report_prompt = prompt.format(
#     repository=report_data["repository"],
#     metrics=report_data["metrics"]
# )

# # buffer_manager.show_loader()
# # result = model.invoke(initial_report_prompt)
# # buffer_manager.hide_loader()

# # buffer_manager.display_content(f"ü§ñ **Git-Bot:** {result.content}")

# # https://github.com/freeCodeCamp/freeCodeCamp
# # https://github.com/midday-ai/midday

def sendSampleResponse():
    return {
        "success": True
    }

# query_classification_prompt = """
# You are given a user query: "{query}". Your task is to classify it with the following details:
#     action: Is the query about a "commits", "pulls", or "issues"?
#     scope: Does the query refer to a "count" (how many result) or "single_document" (specific commit/issue/pull request) or "multiple_documents" (list of them)?
#     state: {{open/close/all}} (omit if action=commit) for "pull_request" and "issue", default=open
#     limit: (omit if not mentioned) Number of results requested by the user.
#     since: (omit if not mentioned) Format {{number}}{{h/d/m/y}}: ('7d' for "7 days" or "last week.", '6m' for "6 months.")
#     author: (omit if not mentioned)
    
#     Output as a JSON object.
# """

# default_response_template = PromptTemplate.from_template("hello world!")

# count_placeholder = "<%COUNT%>"
# count_response_template = PromptTemplate.from_template(f"""
#     Create a professional single sentence stating the count for the given query: "{{query}}"
#     Use exactly one "{count_placeholder}" as a placeholder for the actual count.
# """)

# multiple_docs_placeholder = "<%MULTI_DOC_RESPONSE%>"
# multiple_docs_response_template = PromptTemplate.from_template(f"""
#     Just create an intro and outro stating the list of output for the given query: "{{query}}". No additional text or formatting.
#     outro can be similar to this one, "Let me know if you'd like to know about any specific {{action}}". be creative
#     Use exactly one "{multiple_docs_placeholder}" as a placeholder for the actual response.
#     No need to mention "intro:" or "outro:"
# """)

# async def process_classification(classification): 
#     action = classification.get("action") # "commits", "pulls", "issues"
#     scope = classification.get("scope")   # "count", "single_document", "multiple_documents"
#     state = classification.get("state")
#     since = format_since_datetime(classification.get("since"))
#     author = classification.get("author")
#     per_page = min(10, int(classification.get("limit", 5)))

#     params = {}
#     if isinstance(author, str):
#         params["author"] = author
#     if isinstance(state, str):
#         params["state"] = state
#     if isinstance(since, str):
#         params["since"] = since
         
#     if scope == "count":
#         count = await get_count(action, params)
#         return { "data": count }

#     params["per_page"] = per_page

#     if scope == "multiple_documents":
#         docs = await get_multiple_documents(action, params)
#         return { "data": docs }

#     return sendSampleResponse()

# old="""
# You will be answering questions based on the tools given to you. 
# """

# async def query_resolver(query):
#     buffer_manager = OutputBufferManager()
#     buffer_manager.display_content(f"üë§ **User:** {query}")
    
#     query_classification_prompt_template = ChatPromptTemplate.from_messages([
#          SystemMessage(content=query_classification_prompt),
#          HumanMessage(content=query)                                                  
#     ])
#     # buffer_manager.display_content(query_classification_prompt.format(query=query))
    
#     classifier = query_classification_prompt_template | chat_model | JsonOutputParser()
    
#     buffer_manager.show_loader("Generating classifications...")
#     classification = classifier.invoke({ "query": query })
#     buffer_manager.hide_loader()
    
#     # buffer_manager.display_gitbot_logs(f"**#Classification:** {classification}")
    
#     buffer_manager.show_loader("Generating data from Github API...")
#     response = await process_classification(classification)
#     buffer_manager.hide_loader()
    
#     if classification.get("scope") == "count":
#         chain = count_response_template | chat_model | StrOutputParser()
    
#         buffer_manager.show_loader("Generating response...")
#         placeholder_answer = chain.invoke({ "query": query })
#         answer = placeholder_answer.replace(count_placeholder, str(response["data"]))
#         buffer_manager.hide_loader()
        
#         buffer_manager.display_gitbot_response(answer)
    
#     elif classification.get("scope") == "multiple_documents":
#         chain = multiple_docs_response_template | chat_model | StrOutputParser()
       
#         buffer_manager.show_loader("Generating response...")
#         placeholder_answer = chain.invoke({ "query": query, "action": classification.get("action") })
#         answer = placeholder_answer.replace(multiple_docs_placeholder, f"\n{str(response["data"])}\n")
#         buffer_manager.hide_loader()
    
#         buffer_manager.display_gitbot_response(answer)

# await query_resolver("Can you give me some latest all kinds of pull_requests from last week??")

# await query_resolver("Can you give me last 2 commits??")

# github_agent_prompt = f"""
# You are a GitHub Repository Analysis Agent named GitBot. 
# Answer the following questions as best you can on Github Repository: {github_url}. 
# You have access to the following tools:

# {tools_details}

# Steps:
# 0. Analyze query: Is it GitHub-related? If not, respond as a general AI assistant. If yes, continue to the next steps.
# 1. Rephrase the query if necessary (based on previous context)
# 2. Are there multiple queries involved? If yes, break them down into smaller queries, then continue.
# 3. Is the user asking for a repository summary? If yes, execute the task using the "repository_report_tool". (PAUSE after execution).
# 4. If the user requests specific commit, issue, or pull request information:
#    - First, classify the request:
#      - endpoint: {"commits", "pulls", "issues"}
#      - scope: {"count", "single_document", "multiple_documents"}
#      - state: {"open", "closed", "all"} (None for commits; default "open" for pulls/issues)
#      - limit: [integer] (None if unspecified)
#      - since: [string "{{number}}{{h/d/m/y}}"] (None if unspecified)
#      - author: [string] (None if unspecified)
#    - Execute the task using the "github_tool". (PAUSE after execution).
# 5. Respond: Analyze output, answer clearly and concisely.
# 6. Handle follow-ups, maintain context.

# Data format: 
# - If there is any action needed to perform, must maintain this format. one key value pair per line.
# - As a LOG, If there is any action needed to perform, Show me all the steps: User, Agent Thought, Action, PAUSE
# - If there is no action to perform, return a complete response of the question like an professional markdown response which will be easy to read. No duplication allowed.

# Action: <tool_name>

# Query Classification:
# endpoint: pulls
# scope: count
# state: open
# since: 7d
# limit: 8
# author: Sakib

# PAUSE

# Prioritize accuracy, clarity, and relevance. Output in plain text. 
# """

# multiple query with conjunction.
# Context understanding.
# Handle empty list.

# # Every pull request is an issue, but not every issue is a pull request. For this reason, "shared" actions for both features.

# # 1. The node ID typically starts with PR_, signaling it's a pull request. The node ID typically starts with I_, signaling it's an issue.
# # 2. Both pull requests and issues have a timeline_url to track the events associated with them (comments, status changes, etc.).
# # 3. The html_url contains /pull/ in the path, indicating it is a pull request. The html_url contains /issues/, indicating it is an issue.
# # 4. Presence of the pull_request key. pull_request should be None for issues and should exist for pull requests

# def get_issues_and_pull_requests(params=None):
#     issues_data = fetch_github_data(endpoint="issues", params=params)

#     if not issues_data:
#         logging.error("Failed to fetch issues from the GitHub API.")
#         return []

#     processed_issues = []
#     for issue in issues_data:
#         processed_issues.append({
#             "number": issue["number"],
#             "title": issue["title"],
#             "html_url": issue["html_url"],
#             "state": issue["state"],
#             "state_reason": issue.get("state_reason"),
#             "created_at": issue["created_at"],
#             "updated_at": issue["updated_at"],
#             "closed_at": issue.get("closed_at"),
#             "author_name": issue["user"]["login"],
#             "closed_by": issue.get("closed_by", {}).get("login"),
#             "labels": [label["name"] for label in issue["labels"]],
#             "assignee_name": issue["assignee"]["login"] if issue.get("assignee") else None,
#             "description": issue.get("body"),
#             "reactions": issue.get("reactions"),
#             "comments": issue["comments"],
#             "locked": issue["locked"]
#         })

#     return processed_issues

# # html_url = https://github.com/<owner>/<repo>/<issues or pull>/<number>
# # author_url = https://github.com/<author_name>
# # pull_request = https://github.com/<owner>/<repo>/pull/<number>

# start_time = time.time()

# params = {"state": "all", "since": last_30_days} 
# issues_and_prs = get_issues_and_pull_requests(params=params)

# print(json.dumps(issues_and_prs, indent=4))

# end_time = time.time()
# elapsed_time = end_time - start_time
# print(f"Time taken to fetch issues and pull requests: {elapsed_time:.4f} seconds")

### TODO

1. Work on getting the current version.
2. Chat context understanding...
3. Find how many files are updated in a commit...
4. Issue and PR details api call "on_demand"...
5. Issue and PR timeline api call "on_demand"...
6. Branch details on an commit/pr...
7. Challenges: get count of commits/prs/issues without fetching them all...
8. get branch details and update...

# Plan

- Get repository basic informations.
  [Challenges: Get count of (open/close) (commits/pull requests/issues) without fetching them all]
  [TODO: work on current version]
- Get last 30 days commits/pull requests/issues data to answer on latest data.
- Fine tune model

# last_30_days = (datetime.now() - timedelta(days=30)).isoformat() + "Z"
# last_7_days = (datetime.now() - timedelta(days=7)).isoformat() + "Z"

# pr_numbers = []

# def fetch_and_process_pull_requests(owner, repo):
#     pull_requests = fetch_github_data(endpoint="pulls", repo=f"{owner}/{repo}")

#     processed_prs = []
#     for pr in pull_requests: 
#         processed_prs.append({
#             "pr_title": pr["title"],
#             "pr_url": pr["html_url"],
#             "pr_number": pr["number"],
#             "state": pr["state"],
#             "author_name": pr["user"]["login"],
#             "author_url": pr["user"]["html_url"],
#             "created_at": pr["created_at"],
#             "updated_at": pr["updated_at"],
#             "closed_at": pr["closed_at"],
#             "merged_at": pr["merged_at"],
#             "assignee_name": pr["assignee"]["login"] if pr["assignee"] else None,
#             "assignee_url": pr["assignee"]["html_url"] if pr["assignee"] else None,
#             "requested_reviewers": [
#                 {"reviewer_name": reviewer["login"], "reviewer_url": reviewer["html_url"]}
#                 for reviewer in pr["requested_reviewers"]
#             ],
#             "labels": [label["name"] for label in pr["labels"]],
#             "milestone_title": pr["milestone"]["title"] if pr["milestone"] else None,
#             "commit_sha": pr["head"]["sha"],
#             "commit_ref": pr["head"]["ref"],
#             "base_branch": pr["base"]["ref"],
#             "description": pr["body"],
#             "review_comments_url": pr["review_comments_url"],
#             "commits_url": pr["commits_url"],
#             "statuses_url": pr["statuses_url"],
#             "pr_author_association": pr["author_association"],
#             "__main_from_pr_list": pr
#         })
#         pr_numbers.append(pr["number"])
    
#     return processed_prs

# def get_pull_requests(owner, repo):
#     return get_data_with_backup(f"{owner}_{repo}_pull_requests.csv", lambda: fetch_and_process_pull_requests(owner, repo))

# # pull_requests = get_pull_requests(owner, repo)

# def extract_review_comment_data(review_comment):
#     return {
#         "comment_url": review_comment["url"],
#         "review_id": review_comment["pull_request_review_id"],
#         "comment_id": review_comment["id"],
#         "file_path": review_comment["path"],
#         "comment_body": review_comment["body"],
#         "author_login": review_comment["user"]["login"],
#         "author_url": review_comment["user"]["html_url"],
#         "created_at": review_comment["created_at"],
#         "updated_at": review_comment["updated_at"],
#         "pr_url": review_comment["pull_request_url"]
#     }

# def extract_conversation_data(conversation):
#     return {
#         "comment_url": conversation["url"],
#         "html_url": conversation["html_url"],
#         "author_login": conversation["user"]["login"],
#         "author_url": conversation["user"]["html_url"],
#         "created_at": conversation["created_at"],
#         "updated_at": conversation["updated_at"],
#         "comment_body": conversation["body"]
#     }

# def get_pr_review_comments(owner, repo, pull_number):
#     review_comments = fetch_github_data(endpoint=f"pulls/{pull_number}/comments", repo=f"{owner}/{repo}")
#     if review_comments is not None:
#         return [extract_review_comment_data(comment) for comment in review_comments]
#     else:
#         return []

# def get_pr_conversations(owner, repo, pull_number):
#     conversations = fetch_github_data(endpoint=f"issues/{pull_number}/comments", repo=f"{owner}/{repo}")
#     if conversations is not None:
#         return [extract_conversation_data(conversation) for conversation in conversations]
#     else:
#         return []
        
    # def get_all_pr_data(owner, repo, pr_numbers):
#     pr_review_comments = []
#     pr_conversations = []

#     for pr_number in pr_numbers:
#         pr_review_comments.extend(get_pr_review_comments(owner, repo, pr_number))
#         pr_conversations.extend(get_pr_conversations(owner, repo, pr_number))

#     return pr_review_comments, pr_conversations

# def get_pr_messages(owner, repo):
#     pr_review_comments = []
#     pr_conversations = []
    
#     review_comments_file = f"{owner}_{repo}_pr_review_comments.csv"
#     pr_conversations_file = f"{owner}_{repo}_pr_conversations.csv"

#     is_review_comments_file_exists = os.path.exists(os.path.join(DATA_DIRECTORY, review_comments_file))
#     is_pr_conversations_file_exists = os.path.exists(os.path.join(DATA_DIRECTORY, pr_conversations_file))

#     if is_review_comments_file_exists and is_pr_conversations_file_exists and is_backup_required:
#         logging.info(f"File '{review_comments_file}' exists. Reading data from the file.")
#         pr_review_comments = read_csv_to_array(review_comments_file)
#         pr_conversations = read_csv_to_array(pr_conversations_file)
#     else:
#         logging.info(f"File '{review_comments_file}' does not exist. Fetching data from the GitHub API.")
#         pr_review_comments, pr_conversations = get_all_pr_data(owner, repo, pr_numbers)
#         create_csv_file(pr_review_comments, review_comments_file)
#         create_csv_file(pr_conversations, pr_conversations_file)
#         logging.info(f"{review_comments_file} and {pr_conversations_file} created with backup data.")
    
#     return pr_review_comments, pr_conversations
    